{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb846721",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Hotel Booking Cancellation Prediction\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grid Search Results:\n",
      "                 Model                               Best_Params  Accuracy  \\\n",
      "0  Logistic Regression              {'C': 0.01, 'penalty': 'l2'}    1.0000   \n",
      "1        Decision Tree  {'max_depth': 3, 'min_samples_split': 2}    1.0000   \n",
      "2        Random Forest      {'max_depth': 5, 'n_estimators': 50}    1.0000   \n",
      "3                  KNN                        {'n_neighbors': 3}    0.5824   \n",
      "\n",
      "   Precision  Recall  F1-Score  \n",
      "0     1.0000  1.0000    1.0000  \n",
      "1     1.0000  1.0000    1.0000  \n",
      "2     1.0000  1.0000    1.0000  \n",
      "3     0.4203  0.3361    0.3735  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Rasulbek907\\Desktop\\Hotel Booking Cancellation Prediction\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=5. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Hotel Booking Cancellation Prediction\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Hotel Booking Cancellation Prediction\\mpvenv\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:317: UserWarning: The total space of parameters 4 is smaller than n_iter=5. Running 4 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "[I 2025-12-20 12:23:27,710] A new study created in memory with name: no-name-14a63157-075a-4402-b149-3b72de280182\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Search Results:\n",
      "                 Model                                  Best_Params  Accuracy  \\\n",
      "0  Logistic Regression                                  {'C': 0.01}    1.0000   \n",
      "1        Decision Tree  {'min_samples_split': 5, 'max_depth': None}    1.0000   \n",
      "2        Random Forest     {'n_estimators': 100, 'max_depth': None}    1.0000   \n",
      "3                  KNN                           {'n_neighbors': 3}    0.5824   \n",
      "\n",
      "   Precision  Recall  F1-Score  \n",
      "0     1.0000  1.0000    1.0000  \n",
      "1     1.0000  1.0000    1.0000  \n",
      "2     1.0000  1.0000    1.0000  \n",
      "3     0.4203  0.3361    0.3735  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-12-20 12:23:34,178] Trial 0 finished with value: 1.0 and parameters: {'C': 1.3850598112819665}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:23:40,475] Trial 1 finished with value: 1.0 and parameters: {'C': 0.7198478516676091}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:23:45,839] Trial 2 finished with value: 1.0 and parameters: {'C': 0.926200834625734}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:24:13,564] Trial 3 finished with value: 1.0 and parameters: {'C': 0.0926445595422713}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:24:28,600] Trial 4 finished with value: 1.0 and parameters: {'C': 0.37571782878234855}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:25:01,722] Trial 5 finished with value: 1.0 and parameters: {'C': 0.02597890475663539}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:25:27,490] Trial 6 finished with value: 1.0 and parameters: {'C': 0.09786010867331406}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:25:43,326] Trial 7 finished with value: 1.0 and parameters: {'C': 0.3181769020297283}. Best is trial 0 with value: 1.0.\n",
      "c:\\Users\\Rasulbek907\\Desktop\\Hotel Booking Cancellation Prediction\\mpvenv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:473: ConvergenceWarning: lbfgs failed to converge after 1000 iteration(s) (status=1):\n",
      "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT\n",
      "\n",
      "Increase the number of iterations to improve the convergence (max_iter=1000).\n",
      "You might also want to scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2025-12-20 12:26:16,400] Trial 8 finished with value: 1.0 and parameters: {'C': 0.0449114245476063}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:37,807] Trial 9 finished with value: 1.0 and parameters: {'C': 0.1711964936086629}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:44,059] A new study created in memory with name: no-name-2dd2e1cd-d4a1-43a0-bdb5-14294bc90b4b\n",
      "[I 2025-12-20 12:26:44,472] Trial 0 finished with value: 1.0 and parameters: {'max_depth': 18, 'min_samples_split': 6}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:44,867] Trial 1 finished with value: 1.0 and parameters: {'max_depth': 20, 'min_samples_split': 6}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:45,256] Trial 2 finished with value: 1.0 and parameters: {'max_depth': 9, 'min_samples_split': 4}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:45,641] Trial 3 finished with value: 1.0 and parameters: {'max_depth': 9, 'min_samples_split': 7}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:46,033] Trial 4 finished with value: 1.0 and parameters: {'max_depth': 12, 'min_samples_split': 2}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:46,431] Trial 5 finished with value: 1.0 and parameters: {'max_depth': 2, 'min_samples_split': 4}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:46,813] Trial 6 finished with value: 1.0 and parameters: {'max_depth': 18, 'min_samples_split': 8}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:47,217] Trial 7 finished with value: 1.0 and parameters: {'max_depth': 16, 'min_samples_split': 4}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:47,621] Trial 8 finished with value: 1.0 and parameters: {'max_depth': 15, 'min_samples_split': 5}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:48,057] Trial 9 finished with value: 1.0 and parameters: {'max_depth': 3, 'min_samples_split': 7}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:26:48,552] A new study created in memory with name: no-name-800a83e5-e6b4-4a9c-ac41-bf324e9b3d5b\n",
      "[I 2025-12-20 12:27:10,573] Trial 0 finished with value: 1.0 and parameters: {'n_estimators': 158, 'max_depth': 10}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:27:34,995] Trial 1 finished with value: 1.0 and parameters: {'n_estimators': 173, 'max_depth': 11}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:27:47,194] Trial 2 finished with value: 1.0 and parameters: {'n_estimators': 95, 'max_depth': 7}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:28:12,662] Trial 3 finished with value: 1.0 and parameters: {'n_estimators': 179, 'max_depth': 14}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:28:26,739] Trial 4 finished with value: 1.0 and parameters: {'n_estimators': 96, 'max_depth': 12}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:28:44,278] Trial 5 finished with value: 1.0 and parameters: {'n_estimators': 136, 'max_depth': 8}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:28:50,784] Trial 6 finished with value: 1.0 and parameters: {'n_estimators': 50, 'max_depth': 9}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:29:07,795] Trial 7 finished with value: 1.0 and parameters: {'n_estimators': 110, 'max_depth': 13}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:29:23,216] Trial 8 finished with value: 1.0 and parameters: {'n_estimators': 117, 'max_depth': 8}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:29:41,794] Trial 9 finished with value: 1.0 and parameters: {'n_estimators': 136, 'max_depth': 9}. Best is trial 0 with value: 1.0.\n",
      "[I 2025-12-20 12:30:03,339] A new study created in memory with name: no-name-9fac645d-4e93-4aa6-93e3-6954c89d88c6\n",
      "[I 2025-12-20 12:30:21,687] Trial 0 finished with value: 0.34431741761936785 and parameters: {'n_neighbors': 11}. Best is trial 0 with value: 0.34431741761936785.\n",
      "[I 2025-12-20 12:30:39,867] Trial 1 finished with value: 0.2788309040102711 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: 0.34431741761936785.\n",
      "[I 2025-12-20 12:30:58,099] Trial 2 finished with value: 0.2788309040102711 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: 0.34431741761936785.\n",
      "[I 2025-12-20 12:31:16,361] Trial 3 finished with value: 0.3386547324828001 and parameters: {'n_neighbors': 9}. Best is trial 0 with value: 0.34431741761936785.\n",
      "[I 2025-12-20 12:31:34,377] Trial 4 finished with value: 0.2788309040102711 and parameters: {'n_neighbors': 10}. Best is trial 0 with value: 0.34431741761936785.\n",
      "[I 2025-12-20 12:31:52,656] Trial 5 finished with value: 0.37970300326296863 and parameters: {'n_neighbors': 15}. Best is trial 5 with value: 0.37970300326296863.\n",
      "[I 2025-12-20 12:32:10,598] Trial 6 finished with value: 0.27539779681762544 and parameters: {'n_neighbors': 4}. Best is trial 5 with value: 0.37970300326296863.\n",
      "[I 2025-12-20 12:32:28,702] Trial 7 finished with value: 0.37970300326296863 and parameters: {'n_neighbors': 15}. Best is trial 5 with value: 0.37970300326296863.\n",
      "[I 2025-12-20 12:32:46,903] Trial 8 finished with value: 0.3483012904924941 and parameters: {'n_neighbors': 7}. Best is trial 5 with value: 0.37970300326296863.\n",
      "[I 2025-12-20 12:33:05,020] Trial 9 finished with value: 0.2788309040102711 and parameters: {'n_neighbors': 10}. Best is trial 5 with value: 0.37970300326296863.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna Results:\n",
      "                 Model                                Best_Params  Accuracy  \\\n",
      "0  Logistic Regression                  {'C': 1.3850598112819665}    1.0000   \n",
      "1        Decision Tree  {'max_depth': 18, 'min_samples_split': 6}    1.0000   \n",
      "2        Random Forest     {'n_estimators': 158, 'max_depth': 10}    1.0000   \n",
      "3                  KNN                        {'n_neighbors': 15}    0.6099   \n",
      "\n",
      "   Precision  Recall  F1-Score  \n",
      "0     1.0000  1.0000    1.0000  \n",
      "1     1.0000  1.0000    1.0000  \n",
      "2     1.0000  1.0000    1.0000  \n",
      "3     0.4619  0.3223    0.3797  \n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'C:\\\\Users\\\\Rasulbek907\\\\Desktop\\\\Hotel Booking Cancellation Prediction\\\\Models\\\\0____Logistic_Regression\\n0____Logistic_Regression\\n0____Logistic_Regression\\nName:_Model,_dtype:_object_Tuning_model.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 185\u001b[39m\n\u001b[32m    182\u001b[39m save_path = os.path.join(save_dir, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name.replace(\u001b[33m'\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33m_\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_Tuning_model.pkl\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    184\u001b[39m \u001b[38;5;66;03m# joblib bilan saqlash\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m185\u001b[39m \u001b[43mjoblib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEng yaxshi model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, saqlandi: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# Plotly jadval\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Rasulbek907\\Desktop\\Hotel Booking Cancellation Prediction\\mpvenv\\Lib\\site-packages\\joblib\\numpy_pickle.py:599\u001b[39m, in \u001b[36mdump\u001b[39m\u001b[34m(value, filename, compress, protocol)\u001b[39m\n\u001b[32m    597\u001b[39m         NumpyPickler(f, protocol=protocol).dump(value)\n\u001b[32m    598\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m is_filename:\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mwb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m    600\u001b[39m         NumpyPickler(f, protocol=protocol).dump(value)\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mOSError\u001b[39m: [Errno 22] Invalid argument: 'C:\\\\Users\\\\Rasulbek907\\\\Desktop\\\\Hotel Booking Cancellation Prediction\\\\Models\\\\0____Logistic_Regression\\n0____Logistic_Regression\\n0____Logistic_Regression\\nName:_Model,_dtype:_object_Tuning_model.pkl'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sys, os, logging\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import joblib\n",
    "import plotly.graph_objects as go\n",
    "import optuna\n",
    "\n",
    "# Source papkasini qo'shish\n",
    "source_path = os.path.abspath(\"../Source\")\n",
    "if source_path not in sys.path:\n",
    "    sys.path.append(source_path)\n",
    "from preprocessing import Cleaner, Encoder, Scaler\n",
    "\n",
    "# Logging sozlamalari\n",
    "log_path = r\"C:\\Users\\Rasulbek907\\Desktop\\Hotel Booking Cancellation Prediction\\Log\\data_loader.log\"\n",
    "logging.basicConfig(filename=log_path, filemode='a',\n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    level=logging.INFO)\n",
    "\n",
    "# CSV faylni o'qish\n",
    "csv_path = r\"C:\\Users\\Rasulbek907\\Desktop\\Hotel Booking Cancellation Prediction\\Data\\Raw_Data\\hotel_bookings_updated_2024.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "logging.info(f\"Fayl o'qildi: {len(df)} satr, {len(df.columns)} ustun\")\n",
    "\n",
    "# Target va features\n",
    "y = df['is_canceled']\n",
    "X = df.drop(columns=['is_canceled'])\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Preprocessing\n",
    "cleaner = Cleaner(); cleaner.fit(X_train)\n",
    "X_train_clean = cleaner.transform(X_train)\n",
    "X_test_clean = cleaner.transform(X_test)\n",
    "\n",
    "encoder = Encoder(max_unique=5); encoder.fit(X_train_clean)\n",
    "X_train_enc = encoder.transform(X_train_clean)\n",
    "X_test_enc = encoder.transform(X_test_clean)\n",
    "\n",
    "scaler = Scaler(); scaler.fit(X_train_enc)\n",
    "X_train_final = scaler.transform(X_train_enc)\n",
    "X_test_final = scaler.transform(X_test_enc)\n",
    "\n",
    "# SMOTE bilan balanslash\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_bal, y_train_bal = smote.fit_resample(X_train_final, y_train)\n",
    "\n",
    "# --- 1️⃣ Grid Search Hyperparameter ---\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "param_grid = {\n",
    "    'Logistic Regression': {'C':[0.01,0.1,1,10], 'penalty':['l2']},\n",
    "    'Decision Tree': {'max_depth':[3,5,10,None], 'min_samples_split':[2,5,10]},\n",
    "    'Random Forest': {'n_estimators':[50,100,200], 'max_depth':[5,10,None]},\n",
    "    'KNN': {'n_neighbors':[3,5,7,10]}\n",
    "}\n",
    "\n",
    "grid_models = {'Logistic Regression': lr, 'Decision Tree': dt, 'Random Forest': rf, 'KNN': knn}\n",
    "\n",
    "grid_results = []\n",
    "\n",
    "for name, model in grid_models.items():\n",
    "    grid = GridSearchCV(model, param_grid[name], scoring='f1', cv=5, n_jobs=-1)\n",
    "    grid.fit(X_train_bal, y_train_bal)\n",
    "    y_pred = grid.predict(X_test_final)\n",
    "    grid_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Best_Params\": grid.best_params_,\n",
    "        \"Accuracy\": round(accuracy_score(y_test, y_pred),4),\n",
    "        \"Precision\": round(precision_score(y_test, y_pred),4),\n",
    "        \"Recall\": round(recall_score(y_test, y_pred),4),\n",
    "        \"F1-Score\": round(f1_score(y_test, y_pred),4)\n",
    "    })\n",
    "\n",
    "grid_df = pd.DataFrame(grid_results)\n",
    "print(\"Grid Search Results:\")\n",
    "print(grid_df)\n",
    "\n",
    "# --- 2️⃣ Random Search Hyperparameter ---\n",
    "from scipy.stats import randint\n",
    "\n",
    "param_dist = {\n",
    "    'Logistic Regression': {'C':[0.01,0.1,1,10]},\n",
    "    'Decision Tree': {'max_depth':[3,5,10,None], 'min_samples_split':[2,5,10]},\n",
    "    'Random Forest': {'n_estimators':[50,100,200], 'max_depth':[5,10,None]},\n",
    "    'KNN': {'n_neighbors':[3,5,7,10]}\n",
    "}\n",
    "\n",
    "random_results = []\n",
    "\n",
    "for name, model in grid_models.items():\n",
    "    rand = RandomizedSearchCV(model, param_distributions=param_dist[name], n_iter=5, scoring='f1', cv=5, n_jobs=-1, random_state=42)\n",
    "    rand.fit(X_train_bal, y_train_bal)\n",
    "    y_pred = rand.predict(X_test_final)\n",
    "    random_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Best_Params\": rand.best_params_,\n",
    "        \"Accuracy\": round(accuracy_score(y_test, y_pred),4),\n",
    "        \"Precision\": round(precision_score(y_test, y_pred),4),\n",
    "        \"Recall\": round(recall_score(y_test, y_pred),4),\n",
    "        \"F1-Score\": round(f1_score(y_test, y_pred),4)\n",
    "    })\n",
    "\n",
    "random_df = pd.DataFrame(random_results)\n",
    "print(\"Random Search Results:\")\n",
    "print(random_df)\n",
    "\n",
    "# --- 3️⃣ Optuna Bayesian Hyperparameter Optimization ---\n",
    "optuna_results = []\n",
    "\n",
    "def objective(trial, model_name):\n",
    "    if model_name=='Logistic Regression':\n",
    "        C = trial.suggest_float('C', 0.01, 10.0, log=True)\n",
    "        model = LogisticRegression(C=C, max_iter=1000)\n",
    "    elif model_name=='Decision Tree':\n",
    "        max_depth = trial.suggest_int('max_depth', 2, 20)\n",
    "        min_samples_split = trial.suggest_int('min_samples_split', 2, 10)\n",
    "        model = DecisionTreeClassifier(max_depth=max_depth, min_samples_split=min_samples_split, random_state=42)\n",
    "    elif model_name=='Random Forest':\n",
    "        n_estimators = trial.suggest_int('n_estimators',50,200)\n",
    "        max_depth = trial.suggest_int('max_depth',5,20)\n",
    "        model = RandomForestClassifier(n_estimators=n_estimators, max_depth=max_depth, random_state=42)\n",
    "    elif model_name=='KNN':\n",
    "        n_neighbors = trial.suggest_int('n_neighbors',3,15)\n",
    "        model = KNeighborsClassifier(n_neighbors=n_neighbors)\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "    model.fit(X_train_bal, y_train_bal)\n",
    "    y_pred = model.predict(X_test_final)\n",
    "    return f1_score(y_test, y_pred)\n",
    "\n",
    "for name in grid_models.keys():\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda trial: objective(trial, name), n_trials=10)\n",
    "    best_params = study.best_params\n",
    "    # Train best model\n",
    "    if name=='Logistic Regression':\n",
    "        best_model = LogisticRegression(C=best_params['C'], max_iter=1000)\n",
    "    elif name=='Decision Tree':\n",
    "        best_model = DecisionTreeClassifier(max_depth=best_params['max_depth'], min_samples_split=best_params['min_samples_split'], random_state=42)\n",
    "    elif name=='Random Forest':\n",
    "        best_model = RandomForestClassifier(n_estimators=best_params['n_estimators'], max_depth=best_params['max_depth'], random_state=42)\n",
    "    elif name=='KNN':\n",
    "        best_model = KNeighborsClassifier(n_neighbors=best_params['n_neighbors'])\n",
    "    \n",
    "    best_model.fit(X_train_bal, y_train_bal)\n",
    "    y_pred = best_model.predict(X_test_final)\n",
    "    optuna_results.append({\n",
    "        \"Model\": name,\n",
    "        \"Best_Params\": best_params,\n",
    "        \"Accuracy\": round(accuracy_score(y_test, y_pred),4),\n",
    "        \"Precision\": round(precision_score(y_test, y_pred),4),\n",
    "        \"Recall\": round(recall_score(y_test, y_pred),4),\n",
    "        \"F1-Score\": round(f1_score(y_test, y_pred),4)\n",
    "    })\n",
    "\n",
    "optuna_df = pd.DataFrame(optuna_results)\n",
    "print(\"Optuna Results:\")\n",
    "print(optuna_df)\n",
    "\n",
    "# --- F1 bo'yicha eng yaxshi modelni saqlash ---\n",
    "all_results = pd.concat([grid_df, random_df, optuna_df])\n",
    "best_idx = all_results['F1-Score'].idxmax()\n",
    "best_model_info = all_results.loc[best_idx]\n",
    "best_model_name = str(best_model_info['Model'])\n",
    "# save path\n",
    "save_dir = r\"C:\\Users\\Rasulbek907\\Desktop\\Hotel Booking Cancellation Prediction\\Models\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, f\"{best_model_name.replace(' ','_')}_Tuning_model.pkl\")\n",
    "\n",
    "# joblib bilan saqlash\n",
    "joblib.dump(best_model, save_path)\n",
    "print(f\"Eng yaxshi model: {best_model_name}, saqlandi: {save_path}\")\n",
    "\n",
    "# Plotly jadval\n",
    "colors = []\n",
    "for i, row in all_results.iterrows():\n",
    "    row_colors = []\n",
    "    for metric in ['Accuracy','Precision','Recall','F1-Score']:\n",
    "        if row[metric]>=0.8:\n",
    "            row_colors.append('lightgreen')\n",
    "        elif row[metric]<0.6:\n",
    "            row_colors.append('lightcoral')\n",
    "        else:\n",
    "            row_colors.append('white')\n",
    "    colors.append(['white'] + row_colors)\n",
    "\n",
    "fig = go.Figure(data=[go.Table(\n",
    "    header=dict(values=list(all_results.columns), fill_color='paleturquoise', align='center'),\n",
    "    cells=dict(values=[all_results[col] for col in all_results.columns], fill_color=colors, align='center'))\n",
    "])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7971afd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mpvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
